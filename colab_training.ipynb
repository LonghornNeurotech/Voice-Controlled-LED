{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Voice-Controlled LED: Model Training Notebook\n",
        "\n",
        "This notebook consolidates the entire training pipeline—preprocessing, model architecture, and training—into a single file for easy execution on platforms like Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Dependencies\n",
        "\n",
        "First, we'll install the necessary libraries that are not typically included in Colab's default environment and import all required modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install librosa soundfile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Optional\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Audio Preprocessing and Dataset Loading\n",
        "\n",
        "This section contains all the classes needed for loading audio, applying preprocessing steps, extracting features, and wrapping everything in a PyTorch `Dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Preprocessing Classes ---\n",
        "\n",
        "class AudioPreprocessor:\n",
        "    \"\"\"Base class for audio preprocessing.\"\"\"\n",
        "    def __init__(self, sample_rate: int = 16000):\n",
        "        self.sample_rate = sample_rate\n",
        "    \n",
        "    def load_audio(self, file_path: str) -> np.ndarray:\n",
        "        \"\"\"Load audio file and resample if necessary.\"\"\"\n",
        "        audio, sr = librosa.load(file_path, sr=self.sample_rate)\n",
        "        return audio\n",
        "    \n",
        "    def normalize(self, audio: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Normalize audio signal to [-1, 1] range.\"\"\"\n",
        "        max_val = np.max(np.abs(audio))\n",
        "        if max_val > 0:\n",
        "            return audio / max_val\n",
        "        return audio\n",
        "    \n",
        "    def trim_silence(self, audio: np.ndarray, top_db: int = 20) -> np.ndarray:\n",
        "        \"\"\"Trim silence from beginning and end of audio.\"\"\"\n",
        "        trimmed, _ = librosa.effects.trim(audio, top_db=top_db)\n",
        "        return trimmed\n",
        "\n",
        "class VoicePreprocessor(AudioPreprocessor):\n",
        "    \"\"\"Master preprocessing pipeline for the project.\"\"\"\n",
        "    def __init__(self, sample_rate: int = 16000):\n",
        "        super().__init__(sample_rate)\n",
        "    \n",
        "    def preprocess(self, audio: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Main preprocessing function.\"\"\"\n",
        "        audio = self.normalize(audio)\n",
        "        audio = self.trim_silence(audio)\n",
        "        # Add other preprocessing steps like filtering here if needed\n",
        "        return audio\n",
        "    \n",
        "    def extract_features(self, audio: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Extract MFCC features from audio.\"\"\"\n",
        "        mfccs = librosa.feature.mfcc(\n",
        "            y=audio,\n",
        "            sr=self.sample_rate,\n",
        "            n_mfcc=13,\n",
        "            n_fft=2048,\n",
        "            hop_length=512\n",
        "        )\n",
        "        return mfccs\n",
        "\n",
        "# --- Dataset Classes ---\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    \"\"\"Base dataset class for loading audio files.\"\"\"\n",
        "    def __init__(self, data_dir: str, preprocessor: AudioPreprocessor, transform=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.preprocessor = preprocessor\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self._load_samples()\n",
        "    \n",
        "    def _load_samples(self):\n",
        "        \"\"\"Load all audio samples and their labels.\"\"\"\n",
        "        classes = ['pi_on', 'pi_off', 'background']\n",
        "        label_map = {cls: idx for idx, cls in enumerate(classes)}\n",
        "        \n",
        "        for class_name in classes:\n",
        "            class_dir = self.data_dir / class_name\n",
        "            if class_dir.exists():\n",
        "                for audio_file in class_dir.glob('*.wav'):\n",
        "                    self.samples.append(str(audio_file))\n",
        "                    self.labels.append(label_map[class_name])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio = self.preprocessor.load_audio(self.samples[idx])\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        if self.transform:\n",
        "            audio = self.transform(audio)\n",
        "        \n",
        "        return audio, label\n",
        "\n",
        "class VoiceDataset(AudioDataset):\n",
        "    \"\"\"Master dataset class that applies the full preprocessing pipeline.\"\"\"\n",
        "    def __init__(self, data_dir: str, preprocessor: VoicePreprocessor):\n",
        "        super().__init__(data_dir, preprocessor)\n",
        "        self.preprocessor = preprocessor\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio, label = super().__getitem__(idx)\n",
        "        \n",
        "        # Apply custom preprocessing\n",
        "        processed_audio = self.preprocessor.preprocess(audio)\n",
        "        features = self.preprocessor.extract_features(processed_audio)\n",
        "        \n",
        "        # Convert to tensor\n",
        "        features_tensor = torch.FloatTensor(features)\n",
        "        \n",
        "        return features_tensor, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Defining the Neural Network Architecture\n",
        "\n",
        "Here, we define the PyTorch `nn.Module` that will serve as our voice command classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VoiceClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network model for classifying voice commands.\n",
        "    \n",
        "    Args:\n",
        "        input_size: Size of input features (e.g., flattened MFCC or spectrogram)\n",
        "        num_classes: Number of output classes (default: 3 for pi_on, pi_off, background)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, num_classes: int = 3):\n",
        "        super(VoiceClassifier, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        \n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        \n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "        \n",
        "        self.fc4 = nn.Linear(32, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network. Expects input of shape (batch_size, n_mfcc, time_steps)\n",
        "        \"\"\"\n",
        "        # Flatten the input features\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.dropout3(x)\n",
        "        \n",
        "        x = self.fc4(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Training Pipeline\n",
        "\n",
        "This section includes the functions for a single training epoch and validation, followed by the main training loop logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for data, target in dataloader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "    \n",
        "    return {\n",
        "        'loss': total_loss / len(dataloader),\n",
        "        'accuracy': 100. * correct / total\n",
        "    }\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "    \n",
        "    return {\n",
        "        'loss': total_loss / len(dataloader),\n",
        "        'accuracy': 100. * correct / total\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Configuration and Execution\n",
        "\n",
        "Here we set the hyperparameters, define data paths, and run the complete training process.\n",
        "\n",
        "**ACTION REQUIRED:**\n",
        "1.  Upload your `data` directory (containing `pi_on`, `pi_off`, `background` subfolders) to the Colab session, or mount your Google Drive.\n",
        "2.  Adjust the `DATA_DIR` variable below to point to the correct path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "DATA_DIR = 'data'  # IMPORTANT: Change this to your data directory path\n",
        "MODEL_SAVE_PATH = 'best_model.pt'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "VALIDATION_SPLIT = 0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data Padding ---\n",
        "def pad_collate(batch):\n",
        "    \"\"\"Pads audio features to the max length in a batch.\"\"\"\n",
        "    (features, labels) = zip(*batch)\n",
        "    \n",
        "    # Get sequence lengths\n",
        "    lengths = [f.shape[1] for f in features]\n",
        "    max_len = max(lengths)\n",
        "    \n",
        "    # Pad features\n",
        "    padded_features = torch.zeros(len(features), features[0].shape[0], max_len)\n",
        "    for i, f in enumerate(features):\n",
        "        padded_features[i, :, :f.shape[1]] = f\n",
        "        \n",
        "    labels = torch.LongTensor(labels)\n",
        "    \n",
        "    return padded_features, labels\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Check if data directory exists\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    print(f\"Error: Data directory '{DATA_DIR}' not found.\")\n",
        "    print(\"Please upload your data or mount your Google Drive and update the DATA_DIR variable.\")\n",
        "else:\n",
        "    # Initialize preprocessing and dataset\n",
        "    preprocessor = VoicePreprocessor(sample_rate=16000)\n",
        "    dataset = VoiceDataset(DATA_DIR, preprocessor)\n",
        "    print(f\"Total dataset size: {len(dataset)}\")\n",
        "    \n",
        "    # Split dataset\n",
        "    val_size = int(len(dataset) * VALIDATION_SPLIT)\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate)\n",
        "    print(f\"Train samples: {train_size}, Validation samples: {val_size}\")\n",
        "\n",
        "    # Determine input size for the model from the first batch\n",
        "    # This makes the model adaptable to different audio lengths/feature sizes\n",
        "    try:\n",
        "        temp_features, _ = next(iter(train_loader))\n",
        "        input_size = temp_features.shape[1] * temp_features.shape[2]\n",
        "        print(f\"Determined input size for the model: {input_size}\")\n",
        "        \n",
        "        # Initialize model\n",
        "        model = VoiceClassifier(input_size=input_size, num_classes=3).to(device)\n",
        "        \n",
        "        # Setup training\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "        \n",
        "        # Training loop\n",
        "        best_val_acc = 0.0\n",
        "        print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        for epoch in range(EPOCHS):\n",
        "            train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "            val_metrics = validate(model, val_loader, criterion, device)\n",
        "            \n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "                  f\"Train Loss: {train_metrics['loss']:.4f}, \"\n",
        "                  f\"Train Acc: {train_metrics['accuracy']:.2f}%, \"\n",
        "                  f\"Val Loss: {val_metrics['loss']:.4f}, \"\n",
        "                  f\"Val Acc: {val_metrics['accuracy']:.2f}%\")\n",
        "            \n",
        "            if val_metrics['accuracy'] > best_val_acc:\n",
        "                best_val_acc = val_metrics['accuracy']\n",
        "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "                print(f\"  → Saved best model (val acc: {best_val_acc:.2f}%)\")\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Training complete! Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "        print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    except StopIteration:\n",
        "        print(\"Error: The data loader is empty. This might mean your data directory is empty or misconfigured.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
