{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Voice-Controlled LED: Model Training Notebook\n",
        "\n",
        "**Workflow:**\n",
        "1.  Load 2-second `.wav` files.\n",
        "2.  Convert them to 2D MFCC \"images\".\n",
        "3.  Train a Simple 2D CNN to classify them.\n",
        "4.  Save the quantized model for the Raspberry Pi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Dependencies\n",
        "\n",
        "First, we'll install the necessary libraries and import all required modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install librosa soundfile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Optional\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.signal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Audio Preprocessing and Dataset Loading\n",
        "\n",
        "This section handles loading 2-second audio clips, converting them to MFCCs, and wrapping them in a PyTorch `Dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class AudioPreprocessor:\n",
        "    \"\"\"Base class for audio preprocessing.\"\"\"\n",
        "    def __init__(self, sample_rate: int = 16000):\n",
        "        self.sample_rate = sample_rate\n",
        "    \n",
        "    def load_audio(self, file_path: str) -> np.ndarray:\n",
        "        \"\"\"Load audio file and resample if necessary.\"\"\"\n",
        "        audio, sr = librosa.load(file_path, sr=self.sample_rate)\n",
        "        return audio\n",
        "    \n",
        "    def normalize(self, audio: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Normalize audio signal to [-1, 1] range.\"\"\"\n",
        "        max_val = np.max(np.abs(audio))\n",
        "        if max_val > 0:\n",
        "            return audio / max_val\n",
        "        return audio\n",
        "    \n",
        "    def trim_silence(self, audio: np.ndarray, top_db: int = 20) -> np.ndarray:\n",
        "        \"\"\"Trim silence from beginning and end of audio.\"\"\"\n",
        "        trimmed, _ = librosa.effects.trim(audio, top_db=top_db)\n",
        "        return trimmed\n",
        "    \n",
        "    def butter_bandpass(self, audio, fs, order=5, lowcut=80, highcut=8000):\n",
        "        nyq = 0.5 * fs\n",
        "        low = lowcut / nyq\n",
        "        high = highcut / nyq\n",
        "        b, a = scipy.signal.butter(order, [low, high], btype='band')\n",
        "        y = scipy.signal.lfilter(b, a, audio)\n",
        "        return y\n",
        "\n",
        "class VoicePreprocessor(AudioPreprocessor):\n",
        "    \"\"\"Master preprocessing pipeline for the project.\"\"\"\n",
        "    def __init__(self, sample_rate: int = 16000, n_mfcc: int = 20):\n",
        "        super().__init__(sample_rate)\n",
        "        self.n_mfcc = n_mfcc\n",
        "    \n",
        "    def preprocess(self, audio: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Main preprocessing function.\"\"\"\n",
        "        # Start with baseline preprocessing\n",
        "        # Corrected sampling frequency argument to self.sample_rate\n",
        "        audio = self.butter_bandpass(audio, self.sample_rate)\n",
        "        audio = self.normalize(audio)\n",
        "        audio = self.trim_silence(audio)\n",
        "        \n",
        "        # =================================================================\n",
        "        # TODO - Signal Processing\n",
        "        #\n",
        "        # Experiment with adding signal processing techniques here.\n",
        "        # Examples:\n",
        "        # 1. Bandpass filter to focus on speech frequencies (e.g., 80-8000 Hz)\n",
        "        #    - from scipy.signal import butter, lfilter\n",
        "        # 2. Notch filter to remove powerline noise (e.g., 60 Hz)\n",
        "        #    - from scipy.signal import iirnotch\n",
        "        # 3. Noise reduction algorithms like spectral subtraction.\n",
        "        #\n",
        "        # You can add new methods to this class and call them here.\n",
        "        # =================================================================\n",
        "        \n",
        "        return audio\n",
        "    \n",
        "    def extract_features(self, audio: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Extract MFCC features from audio to create a 2D 'image'.\"\"\"\n",
        "        \n",
        "        # =================================================================\n",
        "        # TODO - Feature Extraction\n",
        "        #\n",
        "        # The current implementation uses only MFCCs. Experiment with\n",
        "        # other feature types or combine them.\n",
        "        #\n",
        "        # 1. MEL-SPECTROGRAM (Good for CNNs):\n",
        "        #    mel_spec = librosa.feature.melspectrogram(y=audio, sr=self.sample_rate, n_mels=128)\n",
        "        #    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "        #    return mel_spec_db\n",
        "        #\n",
        "        # 2. COMBINED FEATURES:\n",
        "        #    - mfccs = librosa.feature.mfcc(...)\n",
        "        #    - chroma = librosa.feature.chroma_stft(...)\n",
        "        #    - combined = np.vstack((mfccs, chroma))\n",
        "        #    - return combined\n",
        "        #\n",
        "        #  adjust the model's input_size if you change the\n",
        "        # feature dimensions\n",
        "        # =================================================================\n",
        "        \n",
        "        # Baseline feature extraction: MFCCs\n",
        "        mfccs = librosa.feature.mfcc(\n",
        "            y=audio,\n",
        "            sr=self.sample_rate,\n",
        "            n_mfcc=self.n_mfcc,\n",
        "            n_fft=400,\n",
        "            hop_length=160\n",
        "        )\n",
        "        return mfccs\n",
        "\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    \"\"\"Base dataset class for loading audio files.\"\"\"\n",
        "    def __init__(self, data_dir: str, preprocessor: AudioPreprocessor, transform=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.preprocessor = preprocessor\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self._load_samples()\n",
        "    \n",
        "    def _load_samples(self):\n",
        "        \"\"\"Load all audio samples and their labels.\"\"\"\n",
        "        classes = ['pi_on', 'pi_off', 'background']\n",
        "        label_map = {cls: idx for idx, cls in enumerate(classes)}\n",
        "        \n",
        "        for class_name in classes:\n",
        "            class_dir = self.data_dir / class_name\n",
        "            if class_dir.exists():\n",
        "                for audio_file in class_dir.glob('*.wav'):\n",
        "                    self.samples.append(str(audio_file))\n",
        "                    self.labels.append(label_map[class_name])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio = self.preprocessor.load_audio(self.samples[idx])\n",
        "        label = self.labels[idx]\n",
        "        return audio, label\n",
        "\n",
        "class VoiceDataset(AudioDataset):\n",
        "    \"\"\"Master dataset class that applies the full preprocessing pipeline.\"\"\"\n",
        "    def __init__(self, data_dir: str, preprocessor: VoicePreprocessor):\n",
        "        super().__init__(data_dir, preprocessor)\n",
        "        self.preprocessor = preprocessor\n",
        "        # Target samples for 2 seconds at 16kHz\n",
        "        self.target_samples = 32000 \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio, label = super().__getitem__(idx)\n",
        "        \n",
        "        # Pad or trim to exact length\n",
        "        if len(audio) < self.target_samples:\n",
        "            audio = np.pad(audio, (0, self.target_samples - len(audio)))\n",
        "        else:\n",
        "            audio = audio[:self.target_samples]\n",
        "        \n",
        "        # Apply custom preprocessing\n",
        "        processed_audio = self.preprocessor.preprocess(audio)\n",
        "        features = self.preprocessor.extract_features(processed_audio)\n",
        "        \n",
        "        # Convert to tensor and add Channel dimension for CNN\n",
        "        # Shape becomes (1, n_mfcc, time_steps)\n",
        "        features_tensor = torch.FloatTensor(features).unsqueeze(0)\n",
        "        \n",
        "        return features_tensor, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Defining the Simple 2D CNN Architecture\n",
        "\n",
        "define a standard 2D Convolutional Neural Network to process the MFCC images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple 2D CNN for audio classification.\n",
        "    Input shape: (batch, 1, n_mfcc, time_steps)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 3):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        # =================================================================\n",
        "        # TODO - Model Architecture\n",
        "        #\n",
        "        # Experiment with the CNN architecture:\n",
        "        # 1. Add more Conv2d layers.\n",
        "        # 2. Change kernel sizes or padding.\n",
        "        # 3. Adjust the size of the fully connected (Linear) layers.\n",
        "        # =================================================================\n",
        "        \n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        \n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(in_features=16 * 5 * 50, out_features=128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Training Pipeline\n",
        "\n",
        "This section includes the functions for training and validating the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for data, target in dataloader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "    \n",
        "    return {\n",
        "        'loss': total_loss / len(dataloader),\n",
        "        'accuracy': 100. * correct / total\n",
        "    }\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "    \n",
        "    return {\n",
        "        'loss': total_loss / len(dataloader),\n",
        "        'accuracy': 100. * correct / total\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Configuration\n",
        "\n",
        "Set hyperparameters, define paths, and run training.\n",
        "\n",
        "1.  Upload your `data` directory to Colab or mount Drive.\n",
        "2.  Adjust `DATA_DIR` to match.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# =================================================================\n",
        "# TODO - Hyperparameter Tuning\n",
        "#\n",
        "# Experiment with epochs, batch size, and learning rate\n",
        "# =================================================================\n",
        "\n",
        "DATA_DIR = '/content/drive/MyDrive/my_recordings'  # Update this path!\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/keyword_model.pth'\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Check data\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    print(f\"Error: Data directory '{DATA_DIR}' not found.\")\n",
        "else:\n",
        "    # Initialize preprocessing and dataset\n",
        "    preprocessor = VoicePreprocessor(sample_rate=16000, n_mfcc=20)\n",
        "    dataset = VoiceDataset(DATA_DIR, preprocessor)\n",
        "    print(f\"Total dataset size: {len(dataset)}\")\n",
        "    \n",
        "    # Split dataset\n",
        "    val_size = int(len(dataset) * VALIDATION_SPLIT)\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = SimpleCNN(num_classes=3).to(device)\n",
        "    \n",
        "    # Setup training\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    # Training loop\n",
        "    best_val_acc = 0.0\n",
        "    print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_metrics = validate(model, val_loader, criterion, device)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "              f\"Train Loss: {train_metrics['loss']:.4f}, \"\n",
        "              f\"Train Acc: {train_metrics['accuracy']:.2f}%, \"\n",
        "              f\"Val Loss: {val_metrics['loss']:.4f}, \"\n",
        "              f\"Val Acc: {val_metrics['accuracy']:.2f}%\")\n",
        "        \n",
        "        if val_metrics['accuracy'] > best_val_acc:\n",
        "            best_val_acc = val_metrics['accuracy']\n",
        "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "            print(f\"  -> Saved best model (val acc: {best_val_acc:.2f}%)\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Training complete! Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
